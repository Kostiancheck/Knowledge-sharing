#spark #hadoop 
Whenever Driver triggers an action Spark builds ***DAG*** (Directed Acyclic Graph) and runs Spark job. The DAG is “directed” because the operations are executed in a specific order, and “acyclic” because there are no loops or cycles in the execution plan. This means that each stage depends on the completion of the previous stage, and each task within a stage can run independently of the other. DAG last "leaf" is an action. After you call an action you cannot add steps to the DAG.

In Spark, the DAG Scheduler is responsible for transforming a sequence of RDD transformations and actions into a directed acyclic graph (DAG)

The DAG (Directed Acyclic Graph) in Spark provides several advantages for the efficient processing of large-scale data. Some of the key advantages of DAG in Spark are:

1. Efficient execution: The DAG allows Spark to break down a large-scale data processing job into smaller, independent tasks that can be executed in parallel. By executing the tasks in parallel, Spark can distribute the workload across multiple machines and perform the job much faster than if it was executed sequentially.
2. Optimization: The DAG allows Spark to optimize the job execution by performing various optimizations, such as pipelining, task reordering, and pruning unnecessary operations. This helps to reduce the overall execution time of the job and improve performance.
3. Fault tolerance: The DAG allows Spark to achieve fault tolerance by using the lineage to recover from node failures during the job execution. This ensures that the job can continue running even if a node fails, without losing any data.
4. Reusability: The DAG allows Spark to reuse the intermediate results generated by a job. This means that if a portion of the data is processed once, it can be reused in subsequent jobs, thereby reducing the processing time and improving performance.
5. Visualization: The DAG provides a visual representation of the logical execution plan of the job, which can help users to better understand the job and identify any potential bottlenecks or performance issues.
![[Spark DAG visualization.png]]

See how Spark uses DAG for the fault tolerance in [[Resilience in RDD]]

### DAG example
```scala
val hist: Map[Int, Long] = sc
 .textFile(inputPath)
 .map(word => (word.toLowerCase(), 1)) 
 .reduceByKey((a, b) => a + b) .map(_.swap) 
 .countByKey() 
```
 
 The first two transformations, map() and reduceByKey(), perform a word count. The third transformation is a map() that swaps the key and value in each pair, to give (count, word) pairs, and the final operation is the countByKey() action, which returns the number of words with each count (i.e., a frequency distribution of word counts). 
 
 Spark’s DAG scheduler turns this job into two stages since the reduceByKey() operation forces a shuffle stage.The resulting DAG is illustrated in Figure 19-2. 
 
 The RDDs within each stage are also, in general, arranged in a DAG. The diagram shows the type of the RDD and the operation that created it. `RDD[String]` was created by textFile(), for instance. To simplify the diagram, some intermediate RDDs generated internally by Spark have been omitted. For example, the RDD returned by textFile() is actually a `MappedRDD[String]` whose parent is a `HadoopRDD[LongWritable, Text]`.
 Notice that the reduceByKey() transformation spans two stages; this is because it is implemented using a shuffle, and the reduce function runs as a combiner on the map side (stage 1) and as a reducer on the reduce side (stage 2) — just like in MapReduce. Also like MapReduce, Spark’s shuffle implementation writes its output to partitioned files on local disk (even for in-memory RDDs), and the files are fetched by the RDD in the next stage.
![[Spark DAG visualization example.png]]

An example of DAG visualization for `sc.parallelize(1 to 100).toDF.count()` from the Spark doc
![[Spark DAG visualization example from doc.png]]
### Sources
1. https://sparkbyexamples.com/spark/what-is-dag-in-spark/?expand_article=1 - sparkbyExample article about DAG
2. High Performance Spark - book, chapter 2
3. [[Hadoop The Definitive Guide, 4th Edition.pdf]] chapter 19
4. https://spark.apache.org/docs/3.4.0/web-ui.html#jobs-detail - spark UI doc
5. [[Driver and Executor]]
6. [[Application, Job, Stage and Task]]