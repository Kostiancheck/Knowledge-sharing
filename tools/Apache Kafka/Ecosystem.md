## Kafka Connect
[Kafka Connect](https://developer.confluent.io/learn-kafka/kafka-connect/intro/?session_ref=https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/) is a system for connecting non-Kafka systems to Kafka in a declarative way, without requiring you to write a bunch of non-differentiated integration code to connect to the same exact systems that the rest of the world is connecting to.

Connect runs as a scalable, fault-tolerant cluster of machines external to the Kafka cluster. Rather than write bespoke code to read data from a database or write messages to Elasticsearch, you deploy pre-built connectors from the extensive connector ecosystem, and configure them with a little bit of JSON. Connect then reads data from source systems and writes it to sink systems automatically.
## Schema 
Schema change is a constant fact of life. Any time you serialize data, put it somewhere, and hope to go get it from that place later on, changes in the format of the data are a perennial challenge. We feel this problem most acutely in database schemas, but message formats in Kafka are no exception. The [Confluent Schema Registry](https://developer.confluent.io/learn-kafka/apache-kafka/schema-registry/?session_ref=https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/) exists to help manage schema change over time. When you release a new producer or a new consumer application with a modified message format, the Schema Registry will help the client application determine whether the new schema is compatible with the old one, given the expectations of other clients that have yet to be versioned. It’s an indispensable tool for a complex deployment.
## Kafka Streams
Producing messages to Kafka is often fairly simple: Messages come from some source, either read from some input or computed from some prior state, and they go into a topic. But reading gets complicated very quickly, and the consumer API really doesn’t offer much more abstraction than the producer.

The [Kafka Streams API](https://developer.confluent.io/learn-kafka/kafka-streams/get-started/?session_ref=https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/) exists to provide this layer of abstraction on top of the vanilla consumer. It’s a Java API that provides a functional view of the typical stream processing primitives that emerge in complex consumers: filtering, grouping, aggregating, joining, and more. It provides an abstraction not just for streams, but for streams turned into tables, and a mechanism for querying those tables as well. It builds on the consumer library’s native horizontal scalability and fault tolerance, while addressing the consumer’s limited support for state management.
## ksqlDB
Writing stream processing applications in Java is a nice thing to do if you’re using Kafka Streams, and if you’re using Java, and if it makes sense to marry stream processing functionality with the application itself. But what if you didn’t want to do those things? Or what if you wanted a simpler approach in which you just used SQL to get your stream processing done?

This is precisely what [ksqlDB](https://ksqldb.io/) is: an application-oriented stream processing database for Kafka. A small cluster of ksqlDB nodes runs continuous stream processing queries written in SQL, constantly consuming input events and producing results back into Kafka. It exposes the same stream and table abstractions as Kafka Streams and makes tables queryable through a lightweight JSON API. Feel free to check out the free introductory course on [ksqlDB](https://developer.confluent.io/learn-kafka/ksqldb/intro/?session_ref=https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/) as well as a more advanced course called [Inside ksqlDB](https://developer.confluent.io/learn-kafka/inside-ksqldb/streaming-architecture/?session_ref=https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/) to learn more.

# Sources
1. https://www.confluent.io/blog/apache-kafka-intro-how-kafka-works/